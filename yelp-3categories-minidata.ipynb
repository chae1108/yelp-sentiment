{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"gensim.models.word2vec\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_directory = os.path.join('dataset')\n",
    "review_filepath = os.path.join(data_directory,'reviews2.json')\n",
    "result_directory = os.path.join('result-3categories-minidata')\n",
    "review_txt_filepath = os.path.join(result_directory,'review_text.txt')\n",
    "star_txt_filepath = os.path.join(result_directory,'star_text.txt')\n",
    "# review_count = 0\n",
    "# with codecs.open(star_txt_filepath, 'w', encoding='utf_8') as star_txt_file:\n",
    "#     with codecs.open(review_txt_filepath, 'w', encoding='utf_8') as review_txt_file:\n",
    "#         with codecs.open(review_filepath, encoding='utf_8') as review_file:\n",
    "#             for review_json in review_file:\n",
    "#                 review = json.loads(review_json)\n",
    "#                 review_txt_file.write(review[u'text'].replace('\\n', ' ').replace('\\r', ' ') + '\\n')\n",
    "#                 if (review[u'stars'] == 1) or (review[u'stars'] == 2):\n",
    "#                     star_txt_file.write('1' + '\\n')\n",
    "#                 if (review[u'stars'] == 3):\n",
    "#                     star_txt_file.write('2' + '\\n')\n",
    "#                 if  (review[u'stars'] == 4) or (review[u'stars'] == 5):\n",
    "#                     star_txt_file.write('3' + '\\n')\n",
    "#                 review_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    for parsed_review in nlp.pipe(line_review(filename), batch_size=10000, n_threads=4):        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = os.path.join(result_directory,'unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "    for sentence in lemmatized_sentence_corpus(review_txt_filepath):\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they have about 20+ thing on tap\n",
      "\n",
      "and if you do not count thing that be not beer e.g. bud they still have about 12+ beer on tap\n",
      "\n",
      "that make me really happy\n",
      "\n",
      "the casual pizza/beer atmosphere be wonderful for a late dinner last night\n",
      "\n",
      "the folk be really nice and the customer be not super crazy either like you would expect sometimes in a college town\n",
      "\n",
      "also they have thing like pool table and arcade machine which make them pretty awesome in my book\n",
      "\n",
      "the pizza be really good\n",
      "\n",
      "we have a special pizza with feta tomato and basil and a basket of fried mushroos and zuccini\n",
      "\n",
      "the wait be reasonable for a pizza make from scratch\n",
      "\n",
      "the pizza be delicious but surprisingly not fill\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print u' '.join(unigram_sentence)\n",
    "    print u''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join(result_directory, 'bigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.43 s, sys: 60.1 ms, total: 4.49 s\n",
      "Wall time: 4.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_model.save(bigram_model_filepath)\n",
    "\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(result_directory,'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for unigram_sentence in unigram_sentences:\n",
    "\n",
    "        bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "\n",
    "        f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they have about 20+ thing on_tap\n",
      "\n",
      "and if_you do not count thing that be not beer e.g. bud they still have about 12+ beer on_tap\n",
      "\n",
      "that make me really happy\n",
      "\n",
      "the casual pizza/beer atmosphere be wonderful for a late dinner last_night\n",
      "\n",
      "the folk be really nice and the customer be not super crazy either like you would expect sometimes in a college_town\n",
      "\n",
      "also they have thing like pool_table and arcade machine which make them pretty awesome in my book\n",
      "\n",
      "the pizza be really good\n",
      "\n",
      "we have a special pizza with feta tomato and basil and a basket of fried mushroos and zuccini\n",
      "\n",
      "the wait be reasonable for a pizza make from_scratch\n",
      "\n",
      "the pizza be delicious but surprisingly not fill\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 230, 240):\n",
    "    print u' '.join(bigram_sentence)\n",
    "    print u''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(result_directory,'trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(result_directory,'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for bigram_sentence in bigram_sentences:\n",
    "\n",
    "        trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "\n",
    "        f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they have about 20+ thing on_tap\n",
      "\n",
      "and if_you do_not count thing that be not beer e.g. bud they still have about 12+ beer_on_tap\n",
      "\n",
      "that make me really happy\n",
      "\n",
      "the casual pizza/beer atmosphere be wonderful for a late dinner last_night\n",
      "\n",
      "the folk be really nice and the customer be not super crazy either like you would expect sometimes in a college_town\n",
      "\n",
      "also they have thing like pool_table and arcade machine which make them pretty awesome in my_book\n",
      "\n",
      "the pizza be really good\n",
      "\n",
      "we have a special pizza with feta tomato and basil and a basket of fried mushroos and zuccini\n",
      "\n",
      "the wait be reasonable for a pizza make from_scratch\n",
      "\n",
      "the pizza be delicious but surprisingly not fill\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 230, 240):\n",
    "    print u' '.join(trigram_sentence)\n",
    "    print u''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join(result_directory,'trigram_transformed_reviews_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for parsed_review in nlp.pipe(line_review(review_txt_filepath),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "\n",
    "        # lemmatize the text, removing punctuation and whitespace\n",
    "        unigram_review = [token.lemma_ for token in parsed_review\n",
    "                          if not punct_space(token)]\n",
    "\n",
    "        # apply the first-order and second-order phrase models\n",
    "        bigram_review = bigram_model[unigram_review]\n",
    "        trigram_review = trigram_model[bigram_review]\n",
    "\n",
    "        # remove any remaining stopwords\n",
    "        trigram_review = [term for term in trigram_review\n",
    "                          if term not in spacy.en.STOPWORDS]\n",
    "\n",
    "        # write the transformed review as a line in the new file\n",
    "        trigram_review = u' '.join(trigram_review)\n",
    "        f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "The Free Speech Movement Cafe has Mario Savio's immortal words etched in metal that you can read while waiting in line:  \"To me, freedom of speech is something that represents the very dignity of what a human being is. ... It is the thing that marks us as just below the angels.\"  That being said, FSM is the second-best cafe in Berkeley to engage in that practice known as \"eye-f*cking.\"  Any student who has ever sat in a cafe to study during midterms has done it.  You're sitting at a table, enjoying your hot grilled chicken panini and fresh green salad, allowing your Americano to cool, when you see an attractive person sitting across the way from you.  S/he is typing away at a laptop, but every so often--so you think--s/he looks up and gazes intently into your eyes.  Maybe s/he is looking THROUGH you to the grayscale images of the Free Speech Movement behind you; maybe the person is simply seeking the camaraderie of another weary, exam-riddled soul.  But you know better.  S/he wants you.  You then allow yourself for the next twenty minutes to highlight the same tired line of Samuel Huntington while eye-f*cking that person.  I've heard this practice referred to in a number of different ways.  For instance, for those inclined towards religion there is \"eye-hand-holding\" (which sounds gross, actually).  For those pursuing MRS degrees there is \"eye-marrying.\"  I prefer the anonymity and ever-so-titillating term mentioned in the paragraph above.  This all being said, FSM is a great place to study and hang out.  It's right in the middle of campus, so it's convenient for mid-day snacks or meetings.  It tends to get very crowded and very hot.  Expect to share your table, unless you do what I sometimes did: turn your table so that it is lengthwise against you.  Then spread out pens, books, and notepads over the entire expanse.  This will give you a little bit of privacy!  And the sandwiches, entrees, and soups are delicious and reasonably priced.  Just be sure not to make a habit of it, as $7 or $8 for a sandwich and coffee every day tends to add up quickly!\n",
      "\n",
      "----\n",
      "\n",
      "Transformed:\n",
      "\n",
      "free_speech_movement_cafe mario savio 's immortal word etch metal read wait line freedom speech represent dignity human_being thing mark angel fsm second best cafe berkeley engage practice know eye f*ck student_who sit cafe study during_midterm you_'re sit table enjoy hot grilled_chicken panini fresh green_salad allow americano cool attractive person sit way s/he type away laptop think s/he look gaze intently eye maybe s/he look grayscale image free_speech_movement maybe person simply seek camaraderie weary exam riddled soul know better s/he want allow twenty_minute highlight tired line samuel huntington eye f*ck person hear practice refer number different way instance inclined religion eye hand hold sound gross actually pursue mrs degree eye marry prefer anonymity titillating term mention paragraph fsm great place study hang_out ' right middle campus ' convenient mid_day snack meeting tend crowded hot expect share table unless_you turn table lengthwise spread pen book notepad entire expanse little_bit privacy sandwich entree soup delicious reasonably_price sure habit $_7 $_8 sandwich coffee every_day tend add quickly\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print u'Original:' + u'\\n'\n",
    "\n",
    "for review in it.islice(line_review(review_txt_filepath), 3645, 3646):\n",
    "    print review\n",
    "\n",
    "print u'----' + u'\\n'\n",
    "print u'Transformed:' + u'\\n'\n",
    "\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 3645, 3646):\n",
    "        print review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_list=pd.read_table(trigram_reviews_filepath, skip_blank_lines=False ,names=['text'])\n",
    "X=X_list.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_list=pd.read_table(star_txt_filepath,names=['stars'])\n",
    "y=y_list.stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17396,)\n",
      "(17396,)\n"
     ]
    }
   ],
   "source": [
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train.values.astype('U'))\n",
    "X_test_dtm = vect.transform(X_test.values.astype('U'))\n",
    "# X_test_dtm2 = vect.transform([\"ddgdhrrrgh00\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4349, 25061)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.711887790297\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_nb = nb.predict(X_test_dtm)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_nb)\n",
    "#print y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.70      0.57      0.63       735\n",
      "          2       0.49      0.37      0.42      1031\n",
      "          3       0.77      0.89      0.83      2583\n",
      "\n",
      "avg / total       0.69      0.71      0.70      4349\n",
      "\n",
      "[[ 422  169  144]\n",
      " [ 118  377  536]\n",
      " [  66  220 2297]]\n"
     ]
    }
   ],
   "source": [
    "print metrics.classification_report(y_test, y_pred_nb)\n",
    "print metrics.confusion_matrix(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5939296389974706"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_binary = np.where(y_test==3, 2, 1)\n",
    "max(y_test_binary.mean(), 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.638537594849\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train_dtm, y_train)\n",
    "y_pred_rf = rf.predict(X_test_dtm)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.657162566107\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svc=svm.SVC(kernel='linear')\n",
    "svc.fit(X_train_dtm, y_train)\n",
    "y_pred_svm = svc.predict(X_test_dtm)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1, 'fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = {'alpha': [0,.5,1],\n",
    "              'fit_prior': [True,False]}\n",
    "nb_rf = GridSearchCV(estimator=nb, param_grid=param_grid, cv=5)\n",
    "nb_rf.fit(X_train_dtm, y_train)\n",
    "nb_rf.best_params_\n",
    "print nb_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.711887790297\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_nb = nb.predict(X_test_dtm)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [10, 50, 100],\n",
    "              'max_depth': [3, 5, 10, None],\n",
    "              'min_samples_split': [2, 5],\n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'bootstrap': [True, False]}\n",
    "gs_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)\n",
    "gs_rf.fit(X_train_dtm, y_train)\n",
    "gs_rf.best_params_\n",
    "print gs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(min_samples_split=2, n_estimators=50, bootstrap=False, criterion='entropy', max_depth=None)\n",
    "rf.fit(X_train_dtm, y_train)\n",
    "y_pred_class = rf.predict(X_test_dtm)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10],\n",
    "              'class_weight': [None,'balanced']}\n",
    "gs_svc = GridSearchCV(svm.SVC(kernel='linear'), param_grid)\n",
    "gs_svc.fit(X_train_dtm, y_train)\n",
    "gs_svc.best_params_\n",
    "print gs_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc=svm.SVC(kernel='linear',C= 0.01, class_weight= 'balanced')\n",
    "svc.fit(X_train_dtm, y_train)\n",
    "y_pred_class = svc.predict(X_test_dtm)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfi = TfidfVectorizer()\n",
    "X_train_tfi = tfi.fit_transform(X_train.values.astype('U'))\n",
    "X_test_tfi = tfi.transform(X_test.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.596229018165\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfi, y_train)\n",
    "y_pred_nb_tfi = nb.predict(X_test_tfi)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_nb_tfi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728213382387\n"
     ]
    }
   ],
   "source": [
    "svc=svm.SVC(kernel='linear')\n",
    "svc.fit(X_train_tfi, y_train)\n",
    "y_pred_svm_tfi = svc.predict(X_test_tfi)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_svm_tfi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr=LogisticRegression()\n",
    "lr.fit(X_train_dtm, y_train)\n",
    "y_pred_lr = lr.predict(X_test_dtm)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1babb0a0fd3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_hv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my_pred_nb_hv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_hv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/naive_bayes.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    550\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    551\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/naive_bayes.pyc\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv=HashingVectorizer()\n",
    "X_train_hv = hv.fit_transform(X_train.values.astype('U'))\n",
    "X_test_hv = hv.transform(X_test.values.astype('U'))\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_hv, y_train)\n",
    "y_pred_nb_hv = nb.predict(X_test_hv)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_nb_hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
