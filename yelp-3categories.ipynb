{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook analyzes data for 100% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"gensim.models.word2vec\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loads Yelp data, saves review texts and ratings in another file.\n",
    "# If stars=1 or 2, store '1'. If stars=3, store '2'. If stars=4 or 5, store '3'\n",
    "\n",
    "data_directory = os.path.join('dataset')\n",
    "review_filepath = os.path.join(data_directory,'yelp_academic_dataset_review.json')\n",
    "result_directory = os.path.join('result-3categories')\n",
    "review_txt_filepath = os.path.join(result_directory,'review_text.txt')\n",
    "star_txt_filepath = os.path.join(result_directory,'star_text.txt')\n",
    "# review_count = 0\n",
    "# with codecs.open(star_txt_filepath, 'w', encoding='utf_8') as star_txt_file:\n",
    "#     with codecs.open(review_txt_filepath, 'w', encoding='utf_8') as review_txt_file:\n",
    "#         with codecs.open(review_filepath, encoding='utf_8') as review_file:\n",
    "#             for review_json in review_file:\n",
    "#                 review = json.loads(review_json)\n",
    "#                 review_txt_file.write(review[u'text'].replace('\\n', ' ').replace('\\r', ' ') + '\\n')\n",
    "#                 if (review[u'stars'] == 1) or (review[u'stars'] == 2):\n",
    "#                     star_txt_file.write('1' + '\\n')\n",
    "#                 if (review[u'stars'] == 3):\n",
    "#                     star_txt_file.write('2' + '\\n')\n",
    "#                 if (review[u'stars'] == 4) or (review[u'stars'] == 5):\n",
    "#                     star_txt_file.write('3' + '\\n')\n",
    "#                 review_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    for parsed_review in nlp.pipe(line_review(filename), batch_size=10000, n_threads=4):        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = os.path.join(result_directory,'unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perform lemmatization\n",
    "with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "    for sentence in lemmatized_sentence_corpus(review_txt_filepath):\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind of like cheers but in a blue collar neighborhood in the 1950 's\n",
      "\n",
      "fan freakin tastic\n",
      "\n",
      "i could feel at home here\n",
      "\n",
      "you definitely want to hit mapquest or plug in your gp though\n",
      "\n",
      "i be not sure that i could find it again on my own it really be a hidden gem\n",
      "\n",
      "i will be make my friend take me back until i can memorize where the heck it be\n",
      "\n",
      "addendum 2nd visit for the fish sandwich\n",
      "\n",
      "excellent\n",
      "\n",
      "truly\n",
      "\n",
      "a pound of fish on a fish shape bun as oppose to da burgh 's seemingly popular hamburger bun\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print u' '.join(unigram_sentence)\n",
    "    print u''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join(result_directory, 'bigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 42s, sys: 4.02 s, total: 8min 46s\n",
      "Wall time: 8min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a bigram model using unigrams\n",
    "\n",
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_model.save(bigram_model_filepath)\n",
    "\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(result_directory,'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for unigram_sentence in unigram_sentences:\n",
    "\n",
    "        bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "\n",
    "        f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind of like cheers but in a blue_collar neighborhood in the 1950_'s\n",
      "\n",
      "fan freakin_tastic\n",
      "\n",
      "i could feel at home here\n",
      "\n",
      "you definitely want to hit mapquest or plug in your gp though\n",
      "\n",
      "i be not sure that i could find it again on my own it really be a hidden_gem\n",
      "\n",
      "i will be make my friend take me back until i can memorize where the heck it be\n",
      "\n",
      "addendum 2nd visit for the fish sandwich\n",
      "\n",
      "excellent\n",
      "\n",
      "truly\n",
      "\n",
      "a pound of fish on a fish shape bun as oppose to da_burgh 's seemingly popular hamburger_bun\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 230, 240):\n",
    "    print u' '.join(bigram_sentence)\n",
    "    print u''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(result_directory,'trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create trigrams based on bigram\n",
    "trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(result_directory,'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for bigram_sentence in bigram_sentences:\n",
    "\n",
    "        trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "\n",
    "        f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind of like cheers but in a blue_collar neighborhood in the 1950_'s\n",
      "\n",
      "fan_freakin_tastic\n",
      "\n",
      "i could feel at home here\n",
      "\n",
      "you definitely want to hit mapquest or plug in your gp though\n",
      "\n",
      "i be not sure that i could find it again on my own it really be a hidden_gem\n",
      "\n",
      "i will be make my friend take me back until i can memorize where the heck it be\n",
      "\n",
      "addendum 2nd visit for the fish sandwich\n",
      "\n",
      "excellent\n",
      "\n",
      "truly\n",
      "\n",
      "a pound of fish on a fish shape bun as oppose to da_burgh 's seemingly popular hamburger_bun\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 230, 240):\n",
    "    print u' '.join(trigram_sentence)\n",
    "    print u''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join(result_directory,'trigram_transformed_reviews_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for parsed_review in nlp.pipe(line_review(review_txt_filepath),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "\n",
    "        # lemmatize the text, removing punctuation and whitespace\n",
    "        unigram_review = [token.lemma_ for token in parsed_review\n",
    "                          if not punct_space(token)]\n",
    "\n",
    "        # apply the first-order and second-order phrase models\n",
    "        bigram_review = bigram_model[unigram_review]\n",
    "        trigram_review = trigram_model[bigram_review]\n",
    "\n",
    "        # remove any remaining stopwords\n",
    "        trigram_review = [term for term in trigram_review\n",
    "                          if term not in spacy.en.STOPWORDS]\n",
    "\n",
    "        # write the transformed review as a line in the new file\n",
    "        trigram_review = u' '.join(trigram_review)\n",
    "        f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "Lucky to have such a wonderful museum in the city I study now, I have spent so many hours wandering around the paintings and the amazing fossils!\n",
      "\n",
      "----\n",
      "\n",
      "Transformed:\n",
      "\n",
      "lucky wonderful museum city study spend hour wander_around painting amazing fossil\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# comparing original review and transformed review\n",
    "print u'Original:' + u'\\n'\n",
    "\n",
    "for review in it.islice(line_review(review_txt_filepath), 3645, 3646):\n",
    "    print review\n",
    "\n",
    "print u'----' + u'\\n'\n",
    "print u'Transformed:' + u'\\n'\n",
    "\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 3645, 3646):\n",
    "        print review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use trigram for X\n",
    "X_list=pd.read_table(trigram_reviews_filepath, skip_blank_lines=False ,names=['text'])\n",
    "X=X_list.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_list=pd.read_table(star_txt_filepath,names=['stars'])\n",
    "y=y_list.stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225213,)\n",
      "(2225213,)\n"
     ]
    }
   ],
   "source": [
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run CountVectorizer on trigram\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train.values.astype('U'))\n",
    "X_test_dtm = vect.transform(X_test.values.astype('U'))\n",
    "# X_test_dtm2 = vect.transform([\"ddgdhrrrgh00\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(556304, 373740)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.775462696655\n"
     ]
    }
   ],
   "source": [
    "# run naive bayse on countvectorizer\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_nb = nb.predict(X_test_dtm)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_nb)\n",
    "#print y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.71      0.68      0.69    112540\n",
      "          2       0.37      0.49      0.42     70773\n",
      "          3       0.90      0.86      0.88    372991\n",
      "\n",
      "avg / total       0.79      0.78      0.78    556304\n",
      "\n",
      "[[ 76737  25453  10350]\n",
      " [ 11568  34332  24873]\n",
      " [ 20318  32349 320324]]\n"
     ]
    }
   ],
   "source": [
    "print metrics.classification_report(y_test, y_pred_nb)\n",
    "print metrics.confusion_matrix(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770729672985\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train_dtm, y_train)\n",
    "y_pred_rf = rf.predict(X_test_dtm)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svc=svm.SVC()\n",
    "svc.fit(X_train_dtm, y_train)\n",
    "y_pred_svm = svc.predict(X_test_dtm)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gridsearch on nb\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = {'alpha': [0,.5,1],\n",
    "              'fit_prior': [True,False]}\n",
    "nb_rf = GridSearchCV(estimator=nb, param_grid=param_grid, cv=5)\n",
    "nb_rf.fit(X_train_dtm, y_train)\n",
    "nb_rf.best_params_\n",
    "print nb_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_nb = nb.predict(X_test_dtm)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gridsearch on randomforest\n",
    "param_grid = {'n_estimators': [10, 50, 100],\n",
    "              'max_depth': [3, 5, 10, None],\n",
    "              'min_samples_split': [2, 5],\n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'bootstrap': [True, False]}\n",
    "gs_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)\n",
    "gs_rf.fit(X_train_dtm, y_train)\n",
    "gs_rf.best_params_\n",
    "print gs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(min_samples_split=2, n_estimators=50, bootstrap=False, criterion='entropy', max_depth=None)\n",
    "rf.fit(X_train_dtm, y_train)\n",
    "y_pred_class = rf.predict(X_test_dtm)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gridsearch on svm\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10],\n",
    "              'class_weight': [None,'balanced']}\n",
    "gs_svc = GridSearchCV(svm.SVC(kernel='linear'), param_grid)\n",
    "gs_svc.fit(X_train_dtm, y_train)\n",
    "gs_svc.best_params_\n",
    "print gs_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc=svm.SVC(kernel='linear',C= 0.01, class_weight= 'balanced')\n",
    "svc.fit(X_train_dtm, y_train)\n",
    "y_pred_class = svc.predict(X_test_dtm)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
